{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11146391,"sourceType":"datasetVersion","datasetId":6953532}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:42:42.905661Z","iopub.execute_input":"2025-03-29T07:42:42.906044Z","iopub.status.idle":"2025-03-29T07:42:43.211278Z","shell.execute_reply.started":"2025-03-29T07:42:42.906012Z","shell.execute_reply":"2025-03-29T07:42:43.210289Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/russian-car-plates-data/sample_submission.csv\n/kaggle/input/russian-car-plates-data/supplemental_english.py\n/kaggle/input/russian-car-plates-data/supplemental_russian.py\n/kaggle/input/russian-car-plates-data/train.csv\n/kaggle/input/russian-car-plates-data/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import re\n\n# # Load dataset\n# df = pd.read_csv(\"/kaggle/input/russian-car-plates-data/train.csv\")\n\n# # ----- DATE FEATURE ENGINEERING -----\n# df[\"date\"] = pd.to_datetime(df[\"date\"])\n# df[\"year\"] = df[\"date\"].dt.year\n# df[\"month\"] = df[\"date\"].dt.month\n# df[\"day\"] = df[\"date\"].dt.day\n# df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n# df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n# df.drop(columns=[\"date\"], inplace=True)  # Remove original date column\n\n# # ----- PLATE FEATURE ENGINEERING -----\n# REGION_MAP = {\n#     \"Republic of Adygea\": [\"01\"],\n#     \"Altai Republic\": [\"04\"],\n#     \"Republic of Bashkortostan\": [\"02\", \"102\", \"702\"],\n#     \"Republic of Buryatia\": [\"03\"],\n#     \"Republic of Dagestan\": [\"05\"],\n#     \"Donetsk People's Republic\": [\"80\", \"180\"],\n#     \"Republic of Ingushetia\": [\"06\"],\n#     \"Kabardino-Balkarian Republic\": [\"07\"],\n#     \"Republic of Kalmykia\": [\"08\"],\n#     \"Karachay-Cherkess Republic\": [\"09\"],\n#     \"Republic of Karelia\": [\"10\"],\n#     \"Komi Republic\": [\"11\"],\n#     \"Republic of Crimea\": [\"82\"],\n#     \"Luhansk People's Republic\": [\"81\", \"181\"],\n#     \"Republic of Mari El\": [\"12\"],\n#     \"Republic of Mordovia\": [\"13\", \"113\"],\n#     \"Sakha Republic\": [\"14\"],\n#     \"Republic of North Ossetia\": [\"15\"],\n#     \"Republic of Tatarstan\": [\"16\", \"116\", \"716\"],\n#     \"Republic of Tyva (Tuva)\": [\"17\"],\n#     \"Udmurt Republic\": [\"18\"],\n#     \"Republic of Khakassia\": [\"19\"],\n#     \"Chechen Republic\": [\"20\", \"95\"],\n#     \"Chuvash Republic\": [\"21\", \"121\"],\n#     \"Altai Krai\": [\"22\", \"122\"],\n#     \"Zabaykalsky Krai\": [\"75\"],\n#     \"Kamchatka Krai\": [\"41\"],\n#     \"Krasnodar Krai\": [\"23\", \"93\", \"123\", \"193\", \"323\"],\n#     \"Krasnoyarsk Krai\": [\"24\", \"84\", \"88\", \"124\"],\n#     \"Perm Krai\": [\"59\", \"81\", \"159\"],\n#     \"Primorsky Krai\": [\"25\", \"125\"],\n#     \"Stavropol Krai\": [\"26\", \"126\"],\n#     \"Khabarovsk Krai\": [\"27\"],\n#     \"Amur Oblast\": [\"28\"],\n#     \"Arkhangelsk Oblast\": [\"29\"],\n#     \"Astrakhan Oblast\": [\"30\", \"130\"],\n#     \"Belgorod Oblast\": [\"31\"],\n#     \"Bryansk Oblast\": [\"32\"],\n#     \"Vladimir Oblast\": [\"33\"],\n#     \"Volgograd Oblast\": [\"34\", \"134\"],\n#     \"Vologda Oblast\": [\"35\"],\n#     \"Voronezh Oblast\": [\"36\", \"136\"],\n#     \"Zaporizhzhia Oblast\": [\"85\", \"185\"],\n#     \"Ivanovo Oblast\": [\"37\"],\n#     \"Irkutsk Oblast\": [\"38\", \"85\", \"138\"],\n#     \"Kaliningrad Oblast\": [\"39\", \"91\"],\n#     \"Kaluga Oblast\": [\"40\"],\n#     \"Kemerovo Oblast\": [\"42\", \"142\"],\n#     \"Kirov Oblast\": [\"43\"],\n#     \"Kostroma Oblast\": [\"44\"],\n#     \"Kurgan Oblast\": [\"45\"],\n#     \"Kursk Oblast\": [\"46\"],\n#     \"Leningrad Oblast\": [\"47\", \"147\"],\n#     \"Lipetsk Oblast\": [\"48\"],\n#     \"Magadan Oblast\": [\"49\"],\n#     \"Moscow Oblast\": [\"50\", \"90\", \"150\", \"190\", \"250\", \"550\", \"750\", \"790\"],\n#     \"Murmansk Oblast\": [\"51\"],\n#     \"Nizhny Novgorod Oblast\": [\"52\", \"152\", \"252\"],\n#     \"Novgorod Oblast\": [\"53\"],\n#     \"Novosibirsk Oblast\": [\"54\", \"154\", \"754\"],\n#     \"Omsk Oblast\": [\"55\", \"155\"],\n#     \"Orenburg Oblast\": [\"56\", \"156\"],\n#     \"Oryol Oblast\": [\"57\"],\n#     \"Penza Oblast\": [\"58\", \"158\"],\n#     \"Pskov Oblast\": [\"60\"],\n#     \"Rostov Oblast\": [\"61\", \"161\", \"761\"],\n#     \"Ryazan Oblast\": [\"62\"],\n#     \"Samara Oblast\": [\"63\", \"163\", \"763\"],\n#     \"Saratov Oblast\": [\"64\", \"164\"],\n#     \"Sakhalin Oblast\": [\"65\"],\n#     \"Sverdlovsk Oblast\": [\"66\", \"96\", \"196\"],\n#     \"Smolensk Oblast\": [\"67\"],\n#     \"Tambov Oblast\": [\"68\"],\n#     \"Tver Oblast\": [\"69\"],\n#     \"Tomsk Oblast\": [\"70\"],\n#     \"Tula Oblast\": [\"71\"],\n#     \"Tyumen Oblast\": [\"72\", \"172\"],\n#     \"Ulyanovsk Oblast\": [\"73\", \"173\"],\n#     \"Kherson Oblast\": [\"84\", \"184\"],\n#     \"Chelyabinsk Oblast\": [\"74\", \"174\", \"774\"],\n#     \"Yaroslavl Oblast\": [\"76\"],\n#     \"Moscow\": [\"77\", \"97\", \"99\", \"177\", \"197\", \"199\", \"777\", \"797\", \"799\", \"977\"],\n#     \"Saint Petersburg\": [\"78\", \"98\", \"178\", \"198\"],\n#     \"Sevastopol\": [\"92\"],\n#     \"Jewish Autonomous Oblast\": [\"79\"],\n#     \"Nenets Autonomous Okrug\": [\"83\"],\n#     \"Khanty-Mansi Autonomous Okrug\": [\"86\", \"186\"],\n#     \"Chukotka Autonomous Okrug\": [\"87\"],\n#     \"Yamalo-Nenets Autonomous Okrug\": [\"89\"],\n#     \"Baikonur\": [\"94\"],\n#     \"Occupational Administration of Kharkiv Oblast\": [\"188\"],\n# }\n\n\n# GOVERNMENT_CODES = {\n#     (\"AMP\", (0, 999), \"97\"): (\"Government of Russia\", 1, 1, 10),\n#     (\"AMP\", (0, 999), \"77\"): (\"Partially Government of Russia\", 0, 1, 8),\n#     (\"EKX\", (0, 999), \"77\"): (\"Partially Federal Protective Service (Federal Protective Service)\", 0, 1, 6),\n#     (\"EKX\", (0, 999), \"97\"): (\"Partially Federal Protective Service (Federal Protective Service)\", 0, 1, 6),\n#     (\"EKX\", (0, 999), \"99\"): (\"Partially Federal Protective Service (Federal Protective Service)\", 0, 1, 6),\n#     (\"KKX\", (0, 999), \"77\"): (\"Partially used on vehicles of Ministry of Security/Federal Counterintelligence Service /Federal Security Service of Russia\", 0, 0, 1),\n#     (\"CAC\", (500, 999), \"77\"): (\"Former officially 'open' plates of Ministry of Security/Federal Counterintelligence Service /Federal Security Service of Russia\", 0, 0, 1),\n#     (\"CAC\", (500, 999), \"77\"): (\"Former officially 'open' plates of Ministry of Security/Federal Counterintelligence Service /Federal Security Service of Russia\", 0, 0, 1),\n#     (\"AOO\", (0, 999), \"77\"): (\"Partially Presidential Administrative Directorate plates\", 0, 1, 6),\n#     (\"BOO\", (0, 999), \"77\"): (\"Partially Presidential Administrative Directorate plates\", 0, 1, 6),\n#     (\"MOO\", (0, 999), \"77\"): (\"Partially Presidential Administrative Directorate plates\", 0, 1, 6),\n#     (\"COO\", (0, 999), \"77\"): (\"Partially Administrative Directorate, Federation Council plates\", 0, 1, 6),\n#     (\"AMM\", (0, 999), \"99\"): (\"Partially plates of Moscow City Duma deputies, police\", 0, 1, 4),\n#     (\"CCC\", (0, 999), \"77\"): (\"Partially Central Special Communication, Courier Service, Ministry of Communications\", 0, 1, 3),\n#     (\"CCC\", (0, 999), \"99\"): (\"Partially Tax Police, Customs, Special Communications\", 0, 1, 3),\n#     (\"CCC\", (0, 999), \"97\"): (\"Partially Central Special Communication, Courier Service, Ministry of Communications\", 0, 1, 3),\n#     (\"KKK\", (0, 999), \"99\"): (\"Initially belonged to Courier Service, now used among private individuals\", 0, 0, 1),\n#     (\"OOO\", (0, 999), \"77\"): (\"Initially intended for Federal Security Service\", 0, 0, 1),\n#     (\"KMM\", (0, 999), \"77\"): (\"Partially Fire Department plates\", 0, 1, 3),\n#     (\"MMP\", (300, 320), \"77\"): (\"Partially Federal Security Service plates\", 0, 1, 4),\n#     (\"MMP\", (0, 299), \"77\"): (\"Partially Government of Russia, Federal Security Service, banks, and private individuals with connections in the traffic police\", 0, 1, 2),\n#     (\"MMP\", (321, 999), \"77\"): (\"Partially Government of Russia, Federal Security Service, banks, and private individuals with connections in the traffic police\", 0, 1, 2),\n#     (\"PMP\", (0, 999), \"77\"): (\"Partially Ministry of Justice plates\", 0, 1, 3),\n#     (\"AMO\", (0, 999), \"77\"): (\"Partially Moscow City Hall plates\", 0, 1, 5),\n#     (\"KOO\", (0, 999), \"77\"): (\"Partially Constitutional Court plates\", 0, 1, 3),\n#     (\"EPE\", (0, 999), \"77\"): (\"Partially State Duma plates\", 0, 1, 3),\n#     (\"AAA\", (0, 999), \"77\"): (\"Partially Administration of the President plates\", 0, 1, 6),\n#     (\"KMP\", (0, 999), \"77\"): (\"Partially Government of Russia plates\", 0, 1, 3),\n#     (\"TMP\", (0, 999), \"77\"): (\"Partially Government of Russia plates, as well as private individuals with connections in the traffic police\", 0, 1, 2),\n#     (\"YMP\", (0, 999), \"77\"): (\"Partially Government of Russia plates, as well as private individuals with connections in the traffic police\", 0, 1, 2),\n#     (\"XXX\", (0, 999), \"77\"): (\"Private individuals with connections in the traffic police\", 0, 1, 2),\n#     (\"YYY\", (0, 999), \"77\"): (\"Private individuals with connections in the traffic police\", 0, 1, 2),\n#     (\"XKX\", (0, 999), \"77\"): (\"Partially Federal Security Service and Federal Protective Service plates\", 0, 1, 2),\n#     (\"OMP\", (0, 999), \"77\"): (\"Partially Government of Russia, banks, and private individuals with connections in the traffic police\", 0, 1, 2),\n#     (\"EEE\", (0, 999), \"77\"): (\"Private individuals with connections in the traffic police\", 0, 1, 2),\n\n#     # Moscow Oblast\n#     (\"AMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"BMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"KMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"CMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"OMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"MMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"TMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"HMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"YMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"XMO\", (0, 999), \"50\"): (\"Partially various government agencies (administration, ambulance, traffic police, etc.)\", 0, 1, 3),\n#     (\"AMM\", (0, 999), \"50\"): (\"Partially plates of the regional administration\", 0, 1, 5),\n#     (\"AMM\", (0, 999), \"90\"): (\"Partially plates of the regional administration\", 0, 1, 5),\n#     (\"MMM\", (0, 999), \"50\"): (\"Partially plates of law enforcement in the region (prosecutor's office, EMERCOM, traffic police, etc.)\", 0, 1, 5),\n#     (\"MMM\", (0, 999), \"90\"): (\"Partially plates of law enforcement in the region (prosecutor's office, EMERCOM, traffic police, etc.)\", 0, 1, 5),\n\n#     # Saint Petersburg\n#     (\"OBO\", (0, 999), \"78\"): (\"Partially Departmental Security Service plates\", 0, 1, 4),\n#     (\"OBO\", (0, 999), \"98\"): (\"Partially Departmental Security Service plates\", 0, 1, 4),\n#     (\"OTT\", (0, 999), \"78\"): (\"Partially former traffic police plates (now replaced by 98)\", 0, 0, 1),\n#     (\"OTT\", (0, 999), \"98\"): (\"Partially traffic police plates\", 0, 1, 4),\n#     (\"OMM\", (0, 999), \"78\"): (\"Partially city district police plates\", 0, 1, 3),\n#     (\"OMM\", (0, 999), \"98\"): (\"Partially city district police plates\", 0, 1, 3),\n#     (\"OOM\", (0, 999), \"78\"): (\"Partially plates of the Main Department of Internal Affairs\", 0, 1, 3),\n#     (\"OOM\", (0, 999), \"98\"): (\"Partially plates of the Main Department of Internal Affairs\", 0, 1, 3),\n#     (\"OKO\", (0, 100), \"78\"): (\"Partially former plates of the prosecutor's office and judicial department (now replaced by 98)\", 0, 0, 1),\n#     (\"OKO\", (0, 100), \"98\"): (\"Partially plates of the prosecutor's office and judicial department\", 0, 1, 3),\n#     (\"OKO\", (700, 999), \"78\"): (\"Partially former Federal Security Service plates (now replaced by 98)\", 0, 0, 1),\n#     (\"OKO\", (700, 999), \"98\"): (\"Partially Federal Security Service plates\", 0, 1, 3),\n#     (\"OPP\", (0, 999), \"78\"): (\"Partially former plates of the Main Department of Internal Affairs (now replaced by 98)\", 0, 0, 1),\n#     (\"OPP\", (0, 999), \"98\"): (\"Partially plates of the Main Department of Internal Affairs\", 0, 1, 3),\n#     (\"OOH\", (0, 999), \"78\"): (\"Partially Federal Drug Control Service and Federal Tax Service plates\", 0, 1, 3),\n#     (\"OOH\", (0, 999), \"98\"): (\"Partially Federal Drug Control Service and Federal Tax Service plates\", 0, 1, 3),\n#     (\"OAO\", (0, 999), \"78\"): (\"Partially plates of the city and regional administration\", 0, 1, 5),\n#     (\"OAO\", (0, 999), \"98\"): (\"Partially plates of the city and regional administration\", 0, 1, 5),\n#     (\"AAA\", (0, 100), \"78\"): (\"Partially plates of the city and regional administration\", 0, 1, 6),\n#     (\"AAA\", (0, 100), \"98\"): (\"Partially plates of the city and regional administration\", 0, 1, 6),\n#     (\"OOO\", (0, 899), \"78\"): (\"Commercial plates\", 0, 0, 2),\n#     (\"OOO\", (0, 899), \"98\"): (\"Commercial plates\", 0, 0, 2),\n#     (\"OOO\", (900, 999), \"78\"): (\"Partially Federal Protective Service plates\", 0, 1, 3),\n#     (\"OOO\", (900, 999), \"98\"): (\"Partially Federal Protective Service plates\", 0, 1, 3),\n#     (\"OKC\", (0, 999), \"98\"): (\"Partially Constitutional Court of the Russian Federation plates\", 0, 1, 3),\n#     (\"OOC\", (0, 999), \"78\"): (\"Partially plates of heads of enterprises and organizations\", 0, 0, 2),\n#     (\"OOC\", (0, 999), \"98\"): (\"Partially plates of heads of enterprises and organizations\", 0, 0, 2),\n#     (\"MMM\", (0, 999), \"78\"): (\"Commercial plates\", 0, 0, 2),\n#     (\"MMM\", (0, 999), \"98\"): (\"Commercial plates\", 0, 0, 2),\n\n#     # Altai Republic\n#     (\"XXX\", (0, 999), \"04\"): (\"Widespread 'special' plates\", 0, 0, 2),\n#     (\"TTT\", (0, 999), \"04\"): (\"Rare 'special' plates\", 0, 0, 2),\n#     (\"PPP\", (0, 999), \"04\"): (\"Partially prosecutor's office of the republic\", 0, 1, 3),\n#     (\"PPA\", (0, 999), \"04\"): (\"Partially prosecutor's office of the republic\", 0, 1, 3),\n#     (\"MPA\", (0, 999), \"04\"): (\"Partially Ministry of Internal Affairs of the republic\", 0, 1, 3),\n#     (\"OOO\", (0, 999), \"04\"): (\"Partially plates of the government of the republic\", 0, 1, 5),\n#     (\"HHH\", (0, 999), \"04\"): (\"Partially the republic's tax service plates\", 0, 1, 3),\n#     (\"CCC\", (0, 999), \"04\"): (\"Partially plates belonging to the republic's judges\", 0, 1, 3),\n\n#     # Republic of Bashkortostan\n#     (\"PKC\", (0, 999), \"02\"): (\"Partially State Assembly (Kurultai) plates\", 0, 1, 5),\n#     (\"KKC\", (0, 999), \"02\"): (\"Partially State Assembly (Kurultai) plates\", 0, 1, 5),\n#     (\"OOO\", (0, 999), \"02\"): (\"Partially plates of leaders of large enterprises and ministries\", 0, 1, 3),\n#     (\"AAA\", (0, 999), \"02\"): (\"Partially plates of the republic's government\", 0, 1, 5),\n\n#     # Republic of Karelia\n#     (\"TTT\", (0, 999), \"10\"): (\"Partially government of the republic and Federal Security Service plates\", 0, 1, 5),\n#     (\"HHH\", (0, 999), \"10\"): (\"Partially plates of city and district administrations of the republic\", 0, 1, 4),\n#     (\"MMM\", (0, 999), \"10\"): (\"Partially plates of the Ministry of Internal Affairs of the republic\", 0, 1, 3),\n#     (\"EMP\", (0, 999), \"10\"): (\"Partially plates of the Ministry of Internal Affairs of the republic\", 0, 1, 3),\n#     (\"CCC\", (0, 999), \"10\"): (\"Partially plates of the prosecutor's office and judges' vehicles\", 0, 1, 3),\n\n#     # Komi Republic\n#     (\"TTT\", (0, 999), \"11\"): (\"Partially government of the republic and Federal Security Service plates\", 0, 1, 5),\n#     (\"OOO\", (0, 999), \"11\"): (\"Widespread semi-special plates, leaders of large industrial companies\", 0, 1, 3),\n\n#     # Sakha Republic\n#     (\"PPP\", (0, 999), \"14\"): (\"Partially plates of the republic's prosecutor's office\", 0, 1, 3),\n#     (\"AAA\", (0, 999), \"14\"): (\"Motor pool of the President, Government, Parliament of the republic, as well as heads of state enterprises\", 0, 1, 5),\n\n#     # Republic of Tatarstan\n#     (\"OAA\", (0, 999), \"16\"): (\"Partially plates of heads of district administrations\", 0, 1, 5),\n#     (\"OAA\", (0, 999), \"116\"): (\"Partially plates of heads of district administrations\", 0, 1, 5),\n#     (\"OAA\", (0, 999), \"716\"): (\"Partially plates of heads of district administrations\", 0, 1, 5),\n\n#     # Krasnodar Krai\n#     (\"PPP\", (0, 999), \"23\"): (\"Partially plates of the Krai and city administrations\", 0, 1, 5),\n#     (\"HHH\", (0, 999), \"23\"): (\"Partially plates of the tax authorities\", 0, 1, 3),\n#     (\"OOO\", (0, 999), \"23\"): (\"Partially plates of the Krai and city administrations\", 0, 1, 5),\n#     (\"KKK\", (0, 999), \"23\"): (\"Partially plates of the Krai administration\", 0, 1, 5),\n\n#     # Krasnoyarsk Krai\n#     (\"KPK\", (0, 999), \"24\"): (\"Partially plates of the Krai administration\", 0, 1, 5),\n#     (\"OOO\", (0, 999), \"24\"): (\"Partially Federal Security Service plates of the Krai\", 0, 1, 3),\n#     (\"MKK\", (0, 999), \"24\"): (\"Partially former plates of the Ministry of Internal Affairs of the Krai\", 0, 0, 1),\n\n#     # Primorsky Krai\n#     (\"BOO\", (0, 999), \"25\"): (\"Partially military plates\", 0, 1, 3),\n#     (\"BOO\", (0, 999), \"125\"): (\"Partially city services plates in Vladivostok and districts\", 0, 1, 2),\n#     (\"AAA\", (0, 999), \"25\"): (\"Issued first in Vladivostok\", 0, 0, 2),\n#     (\"AAA\", (0, 999), \"125\"): (\"One of the most 'special' series, prosecutor's office\", 1, 1, 5),\n#     (\"HHH\", (0, 999), \"25\"): (\"Partially plates of the administration and vehicles of City Duma deputies\", 0, 1, 3),\n#     (\"MMM\", (0, 999), \"25\"): (\"Partially plates of the deputies of the Krai Legislative Assembly\", 0, 1, 3),\n#     (\"CCC\", (0, 999), \"25\"): (\"Partially plates of the Krai administration\", 0, 1, 5),\n#     (\"XXX\", (0, 999), \"25\"): (\"Partially plates of the prosecutor's office and the Department of Internal Affairs\", 0, 1, 2),\n#     (\"OOO\", (0, 999), \"25\"): (\"Partially former plates of the Krai administration (during Governor Evgeny Nazdratenko)\", 0, 0, 1),\n#     (\"TTT\", (0, 999), \"25\"): (\"Partially former plates of the Vladivostok administration and federal agencies in the Krai (during Mayor Yuri Kopylov)\", 0, 0, 1),\n#     (\"MBK\", (0, 999), \"25\"): (\"Partially plates for employees of the Department of Internal Affairs\", 0, 1, 3),\n#     (\"MBK\", (0, 999), \"125\"): (\"Partially plates for employees of the Department of Internal Affairs\", 0, 1, 3),\n#     (\"MOO\", (0, 999), \"25\"): (\"Partially plates for Krai agencies of the Department of Internal Affairs, EMERCOM, firefighters, etc.\", 0, 1, 2),\n#     (\"MOO\", (0, 999), \"125\"): (\"Partially plates for Krai agencies of the Department of Internal Affairs, EMERCOM, firefighters, etc.\", 0, 1, 2),\n#     (\"HOO\", (0, 999), \"25\"): (\"Partially plates of the Department of Internal Affairs, traffic police in the southeastern region of the Krai (Nakhodka)\", 0, 1, 3),\n#     (\"HOO\", (0, 999), \"125\"): (\"Partially plates of the Department of Internal Affairs, traffic police in the southeastern region of the Krai (Nakhodka)\", 0, 1, 3),\n#     (\"YOO\", (0, 999), \"25\"): (\"Partially plates of the Department of Internal Affairs, traffic police in the central region of the Krai (Ussuriysk)\", 0, 1, 3),\n#     (\"YOO\", (0, 999), \"125\"): (\"Partially plates of the Department of Internal Affairs, traffic police in the central region of the Krai (Ussuriysk)\", 0, 1, 3),\n#     (\"COO\", (0, 999), \"25\"): (\"Partially plates of the Department of Internal Affairs, traffic police in the northern region of the Krai (Spassk-Dalny)\", 0, 1, 3),\n#     (\"COO\", (0, 999), \"125\"): (\"Partially plates of the Department of Internal Affairs, traffic police in the northern region of the Krai (Spassk-Dalny)\", 0, 1, 3),\n\n#     # Vologda Oblast\n#     (\"AAA\", (0, 999), \"35\"): (\"Partially plates of the regional government and Vologda city administration\", 0, 1, 5),\n\n#     # Volgograd Oblast\n#     (\"AAM\", (0, 999), \"34\"): (\"Partially plates of the Oblast Duma\", 0, 1, 3),\n#     (\"PAA\", (0, 999), \"34\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"AAA\", (0, 999), \"34\"): (\"Partially plates of the Oblast Prosecutor's Office\", 0, 1, 3),\n#     (\"ACK\", (0, 999), \"34\"): (\"Partially plates of the Investigative Committee, Main Department of Internal Affairs\", 0, 1, 3),\n#     (\"YYY\", (0, 999), \"34\"): (\"Partially Federal Security Service plates\", 0, 1, 3),\n#     (\"AAK\", (0, 999), \"34\"): (\"Partially plates of the Federal Bailiff Service, Ministry of Justice, and Judicial Department\", 0, 1, 3),\n\n#     # Voronezh Oblast\n#     (\"ААА\", (0, 999), \"36\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"BOA\", (0, 999), \"36\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"MMM\", (0, 999), \"36\"): (\"Partially plates of the Oblast Prosecutor's Office\", 0, 1, 3),\n\n#     # Kaliningrad Oblast\n#     (\"AAK\", (0, 999), \"39\"): (\"Partially plates of the Oblast Administration, Federal Security Service, and Prosecutor's Office\", 0, 1, 5),\n#     (\"KKK\", (0, 999), \"39\"): (\"Partially plates of the Oblast Administration, Federal Security Service, and Prosecutor's Office\", 0, 1, 5),\n#     (\"PPP\", (0, 999), \"39\"): (\"Partially former plates of the Oblast Administration (during Governor Boos)\", 0, 0, 1),\n\n#     # Kaluga Oblast\n#     (\"OOO\", (0, 999), \"40\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"TTT\", (0, 999), \"40\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"PPP\", (0, 999), \"40\"): (\"Partially plates of the Oblast Prosecutor's Office\", 0, 1, 3),\n\n#     # Kurgan Oblast\n#     (\"OOO\", (0, 999), \"45\"): (\"Partially former plates of the Oblast Administration\", 0, 0, 1),\n#     (\"TTT\", (0, 999), \"45\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"OKO\", (0, 999), \"45\"): (\"Partially plates of the Oblast Prosecutor's Office\", 0, 1, 3),\n\n#     # Novosibirsk Oblast\n#     (\"AAA\", (0, 199), \"54\"): (\"Plates for Presidential Plenipotentiaries\", 1, 1, 7),\n#     (\"AAA\", (200, 999), \"54\"): (\"'Special' plates\", 0, 1, 4),\n#     (\"HHH\", (0, 999), \"54\"): (\"Partially plates of the Novosibirsk mayor's office, Oblast Administration, and Oblast Council\", 0, 1, 5),\n#     (\"ACK\", (0, 999), \"54\"): (\"Partially Federal Security Service plates of the Oblast\", 0, 1, 3),\n#     (\"AHO\", (0, 999), \"54\"): (\"Partially former plates of the Oblast Administration\", 0, 0, 1),\n#     (\"AAO\", (0, 999), \"54\"): (\"Partially plates of various government agencies, including district administrations of Novosibirsk\", 0, 1, 3),\n#     (\"PPP\", (0, 999), \"54\"): (\"'Morozov' plates (introduced by former head of traffic police Pyotr Morozov)\", 0, 1, 2),\n#     (\"MOP\", (0, 999), \"54\"): (\"'Morozov' plates (introduced by former head of traffic police Pyotr Morozov)\", 0, 1, 2),\n\n#     # Oryol Oblast\n#     (\"AAA\", (0, 999), \"57\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"AOO\", (0, 999), \"57\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"OAO\", (0, 999), \"57\"): (\"Partially plates of directors of public joint-stock companies\", 0, 1, 2),\n\n#     # Rostov Oblast\n#     (\"APO\", (0, 999), \"61\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"AAA\", (0, 999), \"61\"): (\"Partially plates of district heads of Rostov-on-Don, mayors of Oblast cities\", 0, 1, 5),\n#     (\"APY\", (0, 999), \"61\"): (\"Partially plates of the Rostov-on-Don administration\", 0, 1, 5),\n#     (\"KKK\", (0, 999), \"61\"): (\"Partially former plates for Presidential Plenipotentiaries (during Viktor Kazantsev)\", 0, 0, 1),\n#     (\"HHH\", (0, 999), \"61\"): (\"Partially plates of the Oblast Prosecutor's Office\", 0, 1, 3),\n#     (\"MMM\", (0, 999), \"61\"): (\"Partially plates of the Oblast Department of Internal Affairs\", 0, 1, 3),\n#     (\"OOO\", (0, 999), \"61\"): (\"Partially plates of the Oblast Legislative Assembly\", 0, 1, 4),\n#     (\"BBK\", (0, 999), \"61\"): (\"Partially plates of insurance companies in Rostov-on-Don\", 0, 1, 1),\n\n#     # Saratov Oblast\n#     (\"AAA\", (0, 999), \"164\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"PPP\", (0, 999), \"164\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"XXX\", (0, 999), \"64\"): (\"Partially plates of the Oblast courts\", 0, 1, 3),\n#     (\"MMM\", (0, 999), \"64\"): (\"Partially plates of the Oblast Prosecutor's Office\", 0, 1, 3),\n#     (\"OAA\", (0, 999), \"64\"): (\"Partially Federal Security Service plates of the Oblast\", 0, 1, 3),\n\n#     # Tomsk Oblast\n#     (\"ATO\", (0, 999), \"70\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n\n#     # Tyumen Oblast\n#     (\"ATO\", (0, 999), \"72\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"PTO\", (0, 999), \"72\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"MTO\", (0, 999), \"72\"): (\"Partially plates of the Oblast Prosecutor's Office\", 0, 1, 3),\n#     (\"HTO\", (0, 999), \"72\"): (\"Partially plates of the Tax Service\", 0, 1, 3),\n#     (\"CTO\", (0, 999), \"72\"): (\"Partially plates of the Oblast courts\", 0, 1, 3),\n#     (\"YTO\", (0, 999), \"72\"): (\"Partially plates of the bailiff service\", 0, 1, 3),\n#     (\"BAA\", (0, 999), \"72\"): (\"Partially plates of the Oblast Ministry of Internal Affairs\", 0, 1, 3),\n#     (\"KKK\", (0, 999), \"72\"): (\"'Gangster' plates\", 0, 1, 1),\n\n#     # Arkhangelsk Oblast\n#     (\"TTT\", (0, 999), \"29\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"PPP\", (0, 999), \"29\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"MAO\", (0, 999), \"29\"): (\"Partially plates of the Oblast Ministry of Internal Affairs\", 0, 1, 3),\n\n#     # Ryazan Oblast\n#     (\"APO\", (0, 999), \"62\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n\n#     # Samara Oblast\n#     (\"PAA\", (0, 999), \"63\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n#     (\"AAP\", (0, 999), \"63\"): (\"Partially plates of the Oblast Administration\", 0, 1, 5),\n# }\n\n# # Function to get region name from region code\n# def get_region_name(region_code):\n#     for region, codes in REGION_MAP.items():\n#         if region_code in codes:\n#             return region\n#     return \"Unknown\"\n\n# # Modify extraction function to use updated REGION_MAP\n# def extract_plate_features(plate):\n#     pattern = r\"^(?P<first_letter>[A-Z])(?P<num1>\\d{3})(?P<mid_letters>[A-Z]{2})(?P<region>\\d{2,3})$\"\n#     match = re.match(pattern, plate)\n    \n#     if match:\n#         first_letter = match.group(\"first_letter\")\n#         num1 = int(match.group(\"num1\"))\n#         mid_letters = match.group(\"mid_letters\")\n#         region = match.group(\"region\")\n\n#         gov_info = next((info for (letters, num_range, reg), info in GOVERNMENT_CODES.items()\n#                          if letters == mid_letters and num1 in range(num_range[0], num_range[1] + 1) and reg == region),\n#                         (None, 0, 0, 0))  # Default (None, forbidden=0, advantage=0, significance=0)\n\n#         return pd.Series({\n#             \"first_letter\": first_letter,\n#             \"num1\": num1,\n#             \"mid_letters\": mid_letters,\n#             \"region\": region,\n#             \"region_name\": get_region_name(region),\n#             \"is_government_plate\": gov_info[1],  # 1 if forbidden, 0 otherwise\n#             \"has_road_advantage\": gov_info[2],  # 1 if has road advantage, 0 otherwise\n#             \"significance_level\": gov_info[3]   # Custom significance score\n#         })\n#     else:\n#         return pd.Series({\n#             \"first_letter\": np.nan,\n#             \"num1\": np.nan,\n#             \"mid_letters\": np.nan,\n#             \"region\": np.nan,\n#             \"region_name\": \"Unknown\",\n#             \"is_government_plate\": 0,\n#             \"has_road_advantage\": 0,\n#             \"significance_level\": 0\n#         })\n\n\n# # Apply function to extract plate features\n# plate_features = df[\"plate\"].apply(extract_plate_features)\n# df = pd.concat([df, plate_features], axis=1)\n\n# # ----- FEATURE TRANSFORMATION -----\n# # Encoding categorical variables\n# df[\"first_letter\"] = df[\"first_letter\"].astype(\"category\").cat.codes\n# df[\"mid_letters\"] = df[\"mid_letters\"].astype(\"category\").cat.codes\n# df[\"region_name\"] = df[\"region_name\"].astype(\"category\").cat.codes\n\n# # Handle skewed numeric variables (optional)\n# skewed_features = [\"num1\", \"significance_level\"]\n# for feature in skewed_features:\n#     df[feature] = np.log1p(df[feature])  # Log transform to reduce skewness\n\n# # Remove original plate column\n# df.drop(columns=[\"plate\"], inplace=True)\n\n# # Save processed dataset\n# df.to_csv(\"enhanced_train.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:42:43.212532Z","iopub.execute_input":"2025-03-29T07:42:43.212951Z","iopub.status.idle":"2025-03-29T07:42:43.221450Z","shell.execute_reply.started":"2025-03-29T07:42:43.212925Z","shell.execute_reply":"2025-03-29T07:42:43.220265Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Others' FE","metadata":{}},{"cell_type":"code","source":"from shutil import copyfile\n\ncopyfile(src = \"/kaggle/input/russian-car-plates-data/supplemental_english.py\",\n         dst = \"../working/supplemental_english.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:42:43.223128Z","iopub.execute_input":"2025-03-29T07:42:43.223412Z","iopub.status.idle":"2025-03-29T07:42:43.248367Z","shell.execute_reply.started":"2025-03-29T07:42:43.223384Z","shell.execute_reply":"2025-03-29T07:42:43.247328Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'../working/supplemental_english.py'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from supplemental_english import *  # REGION_CODES, GOVERNMENT_CODES\nimport string\n# Ensure the logs directory exists\nos.makedirs(\"./logs\", exist_ok=True)\n\n# Define log file path\nlog_file_path = \"./logs/training_log.txt\"\n\n# Function to log messages to both console and file\ndef printt(message):\n    print(message)\n    with open(log_file_path, \"a\") as log_file:\n        log_file.write(message + \"\\n\")\n\n# SMAPE\ndef smape(y_true, y_pred):\n    y_pred = np.exp(y_pred)  \n    y_true = np.exp(y_true) \n    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100\n\ndef find_importance_values_for_plate(plate: str, gov_codes: dict) -> tuple:\n    letters = plate[0] + plate[4:6]  # Extracts letters\n    numbers = int(plate[1:4])  # Extracts numbers\n    region_code = plate[6:]  # Extracts region code\n\n    # print(plate, \"---\", letters, numbers, region_code)\n    \n    for (code_letters, num_range, region), details in gov_codes.items():\n        if letters == code_letters and region_code == region:\n            if num_range[0] <= numbers <= num_range[1]:  # Checks if within range\n                return (details[2], details[3])  # Importance values\n    \n    return (0, 0)  # Ordinary plate, no government affiliation\n\n\ndef add_advantage_on_road_and_significance(data: pd.DataFrame) -> pd.DataFrame:\n    def apply_helper(row):\n        advantage_on_road, significance = find_importance_values_for_plate(row[\"plate\"], GOVERNMENT_CODES)\n        return pd.Series({\n            \"advantage_on_road\": advantage_on_road,\n            \"significance\": significance,\n        })\n\n    data[[\"advantage_on_road\", \"significance\"]] = data.apply(apply_helper, axis=1)\n    return data\n\ndef encode_plate(plate: str) -> list[int]:\n    encoded = []\n    for char in plate:\n        if char in char2idx:\n            encoded.append(char2idx[char])\n        else:\n            encoded.append(0)\n    return encoded\n\n# Define constants\nPLATE_POSSIBLE_LETTERS = \"ABEKMHOPCTYX\"  # 12 total\nALL_CHARS = PLATE_POSSIBLE_LETTERS + string.digits  # 12 + 10 = 22 total\nRANDOM_STATE = 37\nchar2idx = {c: i for i, c in enumerate(ALL_CHARS)}  # char to identifier map\n\n# preprocess data\ndef get_region_code(plate):\n    region_code = str(int(plate[6:]))\n    for region, codes in REGION_CODES.items():\n        if region_code in codes:\n            return region\n    return \"Unknown\"\n\ndef process_data(csv_link, region_price_dict):\n    \n    # Read CSV file\n    df = pd.read_csv(\n        csv_link,\n        dtype={\n            \"id\": int,\n            \"plate\": str,\n        },\n        parse_dates=[\"date\"],\n    )\n    \n    # Ensure 'date' is in datetime format\n    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    \n    # Extracting date features\n    df[\"year\"] = df[\"date\"].dt.year\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"day\"] = df[\"date\"].dt.day\n    df[\"weekday\"] = df[\"date\"].dt.weekday\n    # Removing unnecessary columns\n    df = df.drop(columns=[\"date\"])\n    df = df.drop(columns=[\"id\"])\n    \n    # Adding features (advantage on road (bool), significance (int))\n    df = add_advantage_on_road_and_significance(df)\n    \n    # Standardizing plate format (ensuring 9-character plates)\n    df[\"plate\"] = df[\"plate\"].apply(lambda plate: plate if len(plate) == 9 else f\"{plate[:6]}0{plate[6:]}\")\n\n    # add region code\n    df[\"region_name\"] = df[\"plate\"].apply(get_region_code).astype(str)\n\n    # add series\n    df[\"plate_number\"] = df[\"plate\"].apply(lambda plate: plate[1:4]).astype(str)\n\n    # add number\n    df[\"plate_series\"] = df[\"plate\"].apply(lambda plate: plate[0]+plate[4:6]).astype(str)\n\n    # add region number\n    df[\"plate_region\"] = df[\"plate\"].apply(lambda plate: plate[6:]).astype(str)\n    \n    df = df.drop(columns=[\"plate\"], errors=\"ignore\")\n\n    # map region average to each records\n    df[\"region_avg_price\"] = df[\"region_name\"].map(region_price_dict)\n\n    # Apply logarithm transformation\n    df['price'] = np.log1p(df['price'])\n    df['region_avg_price'] = np.log1p(df['region_avg_price'])\n        \n    return df\n\n# get dict of average region price\ntrain_link = \"/kaggle/input/russian-car-plates-data/train.csv\"\ndf = pd.read_csv(\n    train_link,\n    dtype={\n        \"id\": int,\n        \"plate\": str,\n    },\n    parse_dates=[\"date\"],\n)\n\ndf[\"region_code\"] = df[\"plate\"].apply(get_region_code)\ndf['region_avg_price'] = df.groupby(\"region_code\")[\"price\"].transform(\"mean\") \nregion_avg_price_dict = df.groupby(\"region_code\")[\"region_avg_price\"].first().to_dict()\n\n# read data\ndataset_link = \"/kaggle/input/russian-car-plates-data/train.csv\"\ntrain_df = process_data(dataset_link, region_avg_price_dict)\ntrain_df.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:42:43.249165Z","iopub.execute_input":"2025-03-29T07:42:43.249412Z","iopub.status.idle":"2025-03-29T07:42:53.833596Z","shell.execute_reply.started":"2025-03-29T07:42:43.249387Z","shell.execute_reply":"2025-03-29T07:42:53.832534Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       price  year  month  day  weekday  advantage_on_road  significance  \\\n0  11.082158  2024     12   26        3                  0             0   \n1  11.512935  2024      7   12        4                  0             0   \n\n     region_name plate_number plate_series plate_region  region_avg_price  \n0         Moscow          059          XCP          797         13.156570  \n1  Moscow Oblast          800          YMH          790         12.689147  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>weekday</th>\n      <th>advantage_on_road</th>\n      <th>significance</th>\n      <th>region_name</th>\n      <th>plate_number</th>\n      <th>plate_series</th>\n      <th>plate_region</th>\n      <th>region_avg_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11.082158</td>\n      <td>2024</td>\n      <td>12</td>\n      <td>26</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Moscow</td>\n      <td>059</td>\n      <td>XCP</td>\n      <td>797</td>\n      <td>13.156570</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11.512935</td>\n      <td>2024</td>\n      <td>7</td>\n      <td>12</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Moscow Oblast</td>\n      <td>800</td>\n      <td>YMH</td>\n      <td>790</td>\n      <td>12.689147</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# modeling\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularPredictor\nfrom autogluon.core.metrics import make_scorer\nimport numpy as np\n\n# Load the data\n# train_df = pd.read_csv('/kaggle/working/enhanced_train.csv')\n\n# Prepare the data\ntarget = 'price'\nfeatures = [col for col in train_df.columns if col not in ['id', 'date', target]]\n\n# Split the data\ntrain_data, test_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Calculate SMAPE\ndef smape(y_true, y_pred):\n    y_pred = np.exp(y_pred)  \n    y_true = np.exp(y_true) \n    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100\n\n# Define smape as metrics for AutoGluon\n# smape_scorer = make_scorer(name='smape1', score_func=smape, greater_is_better=False)\n# Train the model      eval_metric=smape_scorer\npredictor = TabularPredictor(\n    label=target,\n    eval_metric = 'smape'\n)\npredictor.fit(train_data=train_data, time_limit=10800, presets='best_quality')\n\n# Evaluate the model\nperformance = predictor.evaluate(test_data)\nprint(\"Performance metrics:\", performance)\n\ny_pred = predictor.predict(test_data.drop(columns=[target]))\ny_true = test_data[target]\nsmape_score = smape(y_true, y_pred)\nprint(f\"SMAPE: {smape_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:51:45.874911Z","iopub.execute_input":"2025-03-29T07:51:45.875265Z","iopub.status.idle":"2025-03-29T11:22:04.589457Z","shell.execute_reply.started":"2025-03-29T07:51:45.875242Z","shell.execute_reply":"2025-03-29T11:22:04.588019Z"}},"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20250329_075145\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.02 GB / 31.35 GB (92.6%)\nDisk Space Avail:   18.35 GB / 19.52 GB (94.0%)\n===================================================\nPresets specified: ['best_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 2700s of the 10800s of remaining time (25%).\n\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20250329_075145/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_ray_fit pid=1458)\u001b[0m \tRan out of time, early stopping on iteration 7047.\n\u001b[36m(_dystack pid=307)\u001b[0m \t-0.5898\t = Validation score   (-root_mean_squared_error)\n\u001b[36m(_dystack pid=307)\u001b[0m \t596.75s\t = Training   runtime\n\u001b[36m(_dystack pid=307)\u001b[0m \t1.08s\t = Validation runtime\n\u001b[36m(_dystack pid=307)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 191.23s of remaining time.\n\u001b[36m(_dystack pid=307)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.542, 'LightGBM_BAG_L1': 0.375, 'LightGBMXT_BAG_L1': 0.042, 'RandomForestMSE_BAG_L1': 0.042}\n\u001b[36m(_dystack pid=307)\u001b[0m \t-0.5761\t = Validation score   (-root_mean_squared_error)\n\u001b[36m(_dystack pid=307)\u001b[0m \t0.04s\t = Training   runtime\n\u001b[36m(_dystack pid=307)\u001b[0m \t0.0s\t = Validation runtime\n\u001b[36m(_dystack pid=307)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=307)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 191.18s of the 191.13s of remaining time.\n\u001b[36m(_dystack pid=307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\u001b[36m(_dystack pid=1497)\u001b[0m Running DyStack sub-fit ...\n\u001b[36m(_dystack pid=1497)\u001b[0m Beginning AutoGluon training ... Time limit = 900s\n\u001b[36m(_dystack pid=1497)\u001b[0m AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250329_074506/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=1497)\u001b[0m Train Data Rows:    36718\n\u001b[36m(_dystack pid=1497)\u001b[0m Train Data Columns: 11\n\u001b[36m(_dystack pid=1497)\u001b[0m Label Column:       price\n\u001b[36m(_dystack pid=1497)\u001b[0m Problem Type:       regression\n\u001b[36m(_dystack pid=1497)\u001b[0m Preprocessing data ...\n\u001b[36m(_dystack pid=1497)\u001b[0m Using Feature Generators to preprocess the data ...\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \tAvailable Memory:                    29775.40 MB\n\u001b[36m(_dystack pid=1497)\u001b[0m \tTrain Data (Original)  Memory Usage: 10.05 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=1497)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tStage 1 Generators:\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tStage 2 Generators:\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \tStage 3 Generators:\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \tStage 4 Generators:\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \tStage 5 Generators:\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n\u001b[36m(_dystack pid=1497)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t('float', [])  : 1 | ['region_avg_price']\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t('int', [])    : 6 | ['year', 'month', 'day', 'weekday', 'advantage_on_road', ...]\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t('object', []) : 4 | ['region_name', 'plate_number', 'plate_series', 'plate_region']\n\u001b[36m(_dystack pid=1497)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t('category', [])  : 4 | ['region_name', 'plate_number', 'plate_series', 'plate_region']\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t('float', [])     : 1 | ['region_avg_price']\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t('int', [])       : 5 | ['year', 'month', 'day', 'weekday', 'significance']\n\u001b[36m(_dystack pid=1497)\u001b[0m \t\t('int', ['bool']) : 1 | ['advantage_on_road']\n\u001b[36m(_dystack pid=1497)\u001b[0m \t0.2s = Fit runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t11 features in original data used to generate 11 features in processed data.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tTrain Data (Processed) Memory Usage: 1.40 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=1497)\u001b[0m Data preprocessing and feature engineering runtime = 0.26s ...\n\u001b[36m(_dystack pid=1497)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'symmetric_mean_absolute_percentage_error'\n\u001b[36m(_dystack pid=1497)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n\u001b[36m(_dystack pid=1497)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n\u001b[36m(_dystack pid=1497)\u001b[0m User-specified model hyperparameters to be fit:\n\u001b[36m(_dystack pid=1497)\u001b[0m {\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n\u001b[36m(_dystack pid=1497)\u001b[0m }\n\u001b[36m(_dystack pid=1497)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.038\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.0388\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=1586)\u001b[0m [1000]\tvalid_set's l2: 0.401379\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0183812\n\u001b[36m(_ray_fit pid=1585)\u001b[0m [3000]\tvalid_set's l2: 0.395908\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0180877\u001b[32m [repeated 8x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=1587)\u001b[0m [5000]\tvalid_set's l2: 0.345085\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0174879\u001b[32m [repeated 8x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=1753)\u001b[0m [1000]\tvalid_set's l2: 0.429877\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0187362\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=1753)\u001b[0m [3000]\tvalid_set's l2: 0.419648\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0183314\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=1881)\u001b[0m [2000]\tvalid_set's l2: 0.361239\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0177163\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=1881)\u001b[0m [4000]\tvalid_set's l2: 0.36046\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0176499\u001b[32m [repeated 6x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=1914)\u001b[0m [5000]\tvalid_set's l2: 0.408223\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182201\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=307)\u001b[0m \t-0.5826\t = Validation score   (-root_mean_squared_error)\n\u001b[36m(_dystack pid=307)\u001b[0m \t37.18s\t = Training   runtime\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=307)\u001b[0m \t0.56s\t = Validation runtime\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=307)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 126.08s of the 126.03s of remaining time.\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=1914)\u001b[0m [7000]\tvalid_set's l2: 0.409805\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182263\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=2087)\u001b[0m [2000]\tvalid_set's l2: 0.388708\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0179786\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=2087)\u001b[0m [4000]\tvalid_set's l2: 0.385365\tvalid_set's symmetric_mean_absolute_percentage_error: -0.017851\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=307)\u001b[0m \t-0.5821\t = Validation score   (-root_mean_squared_error)\n\u001b[36m(_dystack pid=307)\u001b[0m \t20.31s\t = Training   runtime\n\u001b[36m(_dystack pid=307)\u001b[0m \t0.2s\t = Validation runtime\n\u001b[36m(_dystack pid=307)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 102.70s of the 102.65s of remaining time.\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.0179\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t86.13s\t = Training   runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t42.32s\t = Validation runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 503.35s of the 803.40s of remaining time.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n\u001b[36m(_dystack pid=307)\u001b[0m \t-0.5813\t = Validation score   (-root_mean_squared_error)\n\u001b[36m(_dystack pid=307)\u001b[0m \t49.01s\t = Training   runtime\n\u001b[36m(_dystack pid=307)\u001b[0m \t2.05s\t = Validation runtime\n\u001b[36m(_dystack pid=307)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 50.09s of the 50.04s of remaining time.\n\u001b[36m(_dystack pid=307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.08%)\n\u001b[36m(_ray_fit pid=2413)\u001b[0m \tRan out of time, early stopping on iteration 440.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=2567)\u001b[0m [1000]\tvalid_set's l2: 0.365763\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0172552\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=2567)\u001b[0m [2000]\tvalid_set's l2: 0.364864\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0172237\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_ray_fit pid=2588)\u001b[0m \tRan out of time, early stopping on iteration 466.\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=2798)\u001b[0m [1000]\tvalid_set's l2: 0.372749\tvalid_set's symmetric_mean_absolute_percentage_error: -0.017162\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_ray_fit pid=2722)\u001b[0m \tRan out of time, early stopping on iteration 443.\n\u001b[36m(_ray_fit pid=2752)\u001b[0m \tRan out of time, early stopping on iteration 448.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=2798)\u001b[0m [2000]\tvalid_set's l2: 0.372631\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0170809\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_ray_fit pid=2829)\u001b[0m \tRan out of time, early stopping on iteration 454.\n\u001b[36m(_dystack pid=307)\u001b[0m \t-0.5731\t = Validation score   (-root_mean_squared_error)\n\u001b[36m(_dystack pid=307)\u001b[0m \t67.39s\t = Training   runtime\n\u001b[36m(_dystack pid=307)\u001b[0m \t0.44s\t = Validation runtime\n\u001b[36m(_dystack pid=307)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -19.99s of remaining time.\n\u001b[36m(_dystack pid=307)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L2': 0.615, 'RandomForestMSE_BAG_L2': 0.231, 'LightGBM_BAG_L1': 0.077, 'CatBoost_BAG_L1': 0.077}\n\u001b[36m(_dystack pid=307)\u001b[0m \t-0.5719\t = Validation score   (-root_mean_squared_error)\n\u001b[36m(_dystack pid=307)\u001b[0m \t0.44s\t = Training   runtime\n\u001b[36m(_dystack pid=307)\u001b[0m \t0.02s\t = Validation runtime\n\u001b[36m(_dystack pid=307)\u001b[0m AutoGluon training complete, total runtime = 916.85s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 157.3 rows/s (4590 batch size)\n\u001b[36m(_dystack pid=307)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250329_074254/ds_sub_fit/sub_fit_ho\")\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=2877)\u001b[0m [1000]\tvalid_set's l2: 0.399245\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0177382\n\u001b[36m(_ray_fit pid=2915)\u001b[0m [1000]\tvalid_set's l2: 0.342366\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0171438\n\u001b[36m(_ray_fit pid=2915)\u001b[0m [2000]\tvalid_set's l2: 0.342472\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0170579\n\u001b[36m(_ray_fit pid=2957)\u001b[0m [1000]\tvalid_set's l2: 0.381259\tvalid_set's symmetric_mean_absolute_percentage_error: -0.017456\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=307)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=3060)\u001b[0m [1000]\tvalid_set's l2: 0.369754\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0171483\n\u001b[36m(_ray_fit pid=3060)\u001b[0m [2000]\tvalid_set's l2: 0.368654\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0171047\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.0172\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t87.0s\t = Training   runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t15.85s\t = Validation runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 345.84s of the 645.89s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m Running DyStack sub-fit ...\n\u001b[36m(_dystack pid=3096)\u001b[0m Beginning AutoGluon training ... Time limit = 2700s\n\u001b[36m(_dystack pid=3096)\u001b[0m AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250329_075145/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=3096)\u001b[0m Train Data Rows:    36718\n\u001b[36m(_dystack pid=3096)\u001b[0m Train Data Columns: 11\n\u001b[36m(_dystack pid=3096)\u001b[0m Label Column:       price\n\u001b[36m(_dystack pid=3096)\u001b[0m Problem Type:       regression\n\u001b[36m(_dystack pid=3096)\u001b[0m Preprocessing data ...\n\u001b[36m(_dystack pid=3096)\u001b[0m Using Feature Generators to preprocess the data ...\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \tAvailable Memory:                    29675.97 MB\n\u001b[36m(_dystack pid=3096)\u001b[0m \tTrain Data (Original)  Memory Usage: 10.05 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=3096)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tStage 1 Generators:\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tStage 2 Generators:\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \tStage 3 Generators:\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \tStage 4 Generators:\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \tStage 5 Generators:\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n\u001b[36m(_dystack pid=3096)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t('float', [])  : 1 | ['region_avg_price']\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t('int', [])    : 6 | ['year', 'month', 'day', 'weekday', 'advantage_on_road', ...]\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t('object', []) : 4 | ['region_name', 'plate_number', 'plate_series', 'plate_region']\n\u001b[36m(_dystack pid=3096)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t('category', [])  : 4 | ['region_name', 'plate_number', 'plate_series', 'plate_region']\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t('float', [])     : 1 | ['region_avg_price']\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t('int', [])       : 5 | ['year', 'month', 'day', 'weekday', 'significance']\n\u001b[36m(_dystack pid=3096)\u001b[0m \t\t('int', ['bool']) : 1 | ['advantage_on_road']\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.4s = Fit runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t11 features in original data used to generate 11 features in processed data.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tTrain Data (Processed) Memory Usage: 1.40 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=3096)\u001b[0m Data preprocessing and feature engineering runtime = 0.43s ...\n\u001b[36m(_dystack pid=3096)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'symmetric_mean_absolute_percentage_error'\n\u001b[36m(_dystack pid=3096)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n\u001b[36m(_dystack pid=3096)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n\u001b[36m(_dystack pid=3096)\u001b[0m User-specified model hyperparameters to be fit:\n\u001b[36m(_dystack pid=3096)\u001b[0m {\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n\u001b[36m(_dystack pid=3096)\u001b[0m }\n\u001b[36m(_dystack pid=3096)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.038\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.72s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.34s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1795.02s of the 2695.32s of remaining time.\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0388\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.1s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.49s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 324.70s of the 624.75s of remaining time.\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.022\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t18.05s\t = Training   runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t1.84s\t = Validation runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=3160)\u001b[0m [1000]\tvalid_set's l2: 0.403612\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0184188\n\u001b[36m(_ray_fit pid=3159)\u001b[0m [1000]\tvalid_set's l2: 0.401379\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0183812\n\u001b[36m(_ray_fit pid=3160)\u001b[0m [3000]\tvalid_set's l2: 0.395908\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0180877\u001b[32m [repeated 7x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3160)\u001b[0m [5000]\tvalid_set's l2: 0.396097\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0180312\u001b[32m [repeated 8x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3326)\u001b[0m [1000]\tvalid_set's l2: 0.429877\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0187362\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3326)\u001b[0m [3000]\tvalid_set's l2: 0.419648\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0183314\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3395)\u001b[0m [1000]\tvalid_set's l2: 0.418022\tvalid_set's symmetric_mean_absolute_percentage_error: -0.018666\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3395)\u001b[0m [3000]\tvalid_set's l2: 0.408359\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182668\u001b[32m [repeated 6x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3395)\u001b[0m [5000]\tvalid_set's l2: 0.408223\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182201\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3395)\u001b[0m [7000]\tvalid_set's l2: 0.409805\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0182263\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3436)\u001b[0m [2000]\tvalid_set's l2: 0.388708\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0179786\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3436)\u001b[0m [4000]\tvalid_set's l2: 0.385365\tvalid_set's symmetric_mean_absolute_percentage_error: -0.017851\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0179\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t82.27s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t42.48s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 1692.38s of the 2592.67s of remaining time.\n\u001b[36m(_ray_fit pid=3293)\u001b[0m \tRan out of time, early stopping on iteration 3554.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=3603)\u001b[0m [1000]\tvalid_set's l2: 0.365763\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0172552\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3603)\u001b[0m [2000]\tvalid_set's l2: 0.364864\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0172237\n\u001b[36m(_ray_fit pid=3641)\u001b[0m [1000]\tvalid_set's l2: 0.325116\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0168108\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_ray_fit pid=3468)\u001b[0m \tRan out of time, early stopping on iteration 3550.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=3677)\u001b[0m [1000]\tvalid_set's l2: 0.34637\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0169369\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_ray_fit pid=3508)\u001b[0m \tRan out of time, early stopping on iteration 3514.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=3714)\u001b[0m [1000]\tvalid_set's l2: 0.372749\tvalid_set's symmetric_mean_absolute_percentage_error: -0.017162\n\u001b[36m(_ray_fit pid=3714)\u001b[0m [2000]\tvalid_set's l2: 0.372631\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0170809\n\u001b[36m(_ray_fit pid=3758)\u001b[0m [1000]\tvalid_set's l2: 0.399245\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0177382\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_ray_fit pid=3546)\u001b[0m \tRan out of time, early stopping on iteration 3551.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=3789)\u001b[0m [2000]\tvalid_set's l2: 0.342472\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0170579\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3834)\u001b[0m [1000]\tvalid_set's l2: 0.369754\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0171483\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=3834)\u001b[0m [2000]\tvalid_set's l2: 0.368654\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0171047\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0172\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t87.45s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t14.76s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1534.29s of the 2434.59s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.022\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t36.79s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t3.49s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 1491.47s of the 2391.76s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\u001b[36m(_ray_fit pid=3898)\u001b[0m \tRan out of time, early stopping on iteration 2926.\n\u001b[36m(_ray_fit pid=3938)\u001b[0m \tRan out of time, early stopping on iteration 3016.\n\u001b[36m(_ray_fit pid=3968)\u001b[0m \tRan out of time, early stopping on iteration 2944.\n\u001b[36m(_ray_fit pid=4009)\u001b[0m \tRan out of time, early stopping on iteration 2984.\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.017\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t358.84s\t = Training   runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t1.18s\t = Validation runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 241.53s of remaining time.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.478, 'LightGBM_BAG_L1': 0.435, 'LightGBMXT_BAG_L1': 0.043, 'RandomForestMSE_BAG_L1': 0.043}\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.0164\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t0.63s\t = Training   runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t0.0s\t = Validation runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 240.88s of the 240.82s of remaining time.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.0165\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t273.46s\t = Training   runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t0.68s\t = Validation runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -192.24s of remaining time.\n\u001b[36m(_dystack pid=1497)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.375, 'LightGBM_BAG_L1': 0.292, 'CatBoost_BAG_L1': 0.292, 'RandomForestMSE_BAG_L1': 0.042}\n\u001b[36m(_dystack pid=1497)\u001b[0m \t-0.0163\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=1497)\u001b[0m \t0.22s\t = Training   runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m \t0.02s\t = Validation runtime\n\u001b[36m(_dystack pid=1497)\u001b[0m AutoGluon training complete, total runtime = 1092.5s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 76.1 rows/s (4590 batch size)\n\u001b[36m(_dystack pid=1497)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250329_074506/ds_sub_fit/sub_fit_ho\")\n\u001b[36m(_dystack pid=1497)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.017\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t647.49s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t1.37s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 765.10s of the 1665.40s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0246\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t6.68s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t1.74s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 751.25s of the 1651.55s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.06%)\n\u001b[36m(_ray_fit pid=4887)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=4886)\u001b[0m No improvement since epoch 6: early stopping\n\u001b[36m(_ray_fit pid=4886)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=5083)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=5130)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=5083)\u001b[0m No improvement since epoch 9: early stopping\n\u001b[36m(_ray_fit pid=5129)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0193\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t158.01s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.96s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 590.48s of the 1490.77s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.06%)\n\u001b[36m(_ray_fit pid=5187)\u001b[0m No improvement since epoch 7: early stopping\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.018\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t163.7s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t3.63s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 423.56s of the 1323.85s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0179\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t260.67s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.33s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 160.23s of the 1060.53s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=5918)\u001b[0m [1000]\tvalid_set's l2: 0.333541\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0163657\n\u001b[36m(_ray_fit pid=5918)\u001b[0m [2000]\tvalid_set's l2: 0.335069\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0163406\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=5921)\u001b[0m [3000]\tvalid_set's l2: 0.30935\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0161655\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=6056)\u001b[0m [1000]\tvalid_set's l2: 0.386219\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0171679\n\u001b[36m(_ray_fit pid=6090)\u001b[0m [1000]\tvalid_set's l2: 0.329238\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0165261\n\u001b[36m(_ray_fit pid=6126)\u001b[0m [1000]\tvalid_set's l2: 0.364696\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0168396\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=6162)\u001b[0m [1000]\tvalid_set's l2: 0.351952\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0165197\n\u001b[36m(_ray_fit pid=6126)\u001b[0m [2000]\tvalid_set's l2: 0.365168\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0167824\n\u001b[36m(_ray_fit pid=6162)\u001b[0m [2000]\tvalid_set's l2: 0.353336\tvalid_set's symmetric_mean_absolute_percentage_error: -0.0165044\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0166\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t101.44s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t74.23s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 46.95s of the 947.24s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\u001b[36m(_ray_fit pid=6202)\u001b[0m \tRan out of time, early stopping on iteration 590.\n\u001b[36m(_ray_fit pid=6369)\u001b[0m \tRan out of time, early stopping on iteration 573.\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0174\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t41.63s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.51s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 2.56s of the 902.86s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L1.\n\u001b[36m(_ray_fit pid=6367)\u001b[0m \tRan out of time, early stopping on iteration 562.\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 894.41s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tEnsemble Weights: {'LightGBMLarge_BAG_L1': 0.5, 'CatBoost_BAG_L1': 0.312, 'NeuralNetTorch_BAG_L1': 0.125, 'NeuralNetFastAI_BAG_L1': 0.062}\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.12s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.0s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 894.27s of the 894.22s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.09%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0162\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t10.53s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.43s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 881.39s of the 881.34s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.09%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0163\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t10.9s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.24s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 867.43s of the 867.38s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0162\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t78.98s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t2.08s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 783.24s of the 783.19s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.10%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t82.02s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.25s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 698.63s of the 698.59s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0161\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t17.94s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t2.11s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 673.40s of the 673.35s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.13%)\n\u001b[36m(_ray_fit pid=7642)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=7642)\u001b[0m No improvement since epoch 0: early stopping\n\u001b[36m(_ray_fit pid=7640)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=7643)\u001b[0m No improvement since epoch 0: early stopping\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=7835)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=7867)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=7867)\u001b[0m No improvement since epoch 0: early stopping\n\u001b[36m(_ray_fit pid=7897)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=7835)\u001b[0m No improvement since epoch 0: early stopping\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0165\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t121.6s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t1.19s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 549.01s of the 548.96s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.13%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t22.37s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.32s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 523.96s of the 523.91s of remaining time.\n\u001b[36m(_ray_fit pid=7897)\u001b[0m No improvement since epoch 0: early stopping\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0162\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t140.44s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.38s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 380.66s of the 380.61s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.13%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0163\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t15.46s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.61s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 362.09s of the 362.04s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.10%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t56.41s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.21s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 302.39s of the 302.34s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0161\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t183.52s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.37s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 116.09s of the 116.04s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.10%)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0162\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t15.7s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t1.34s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 97.18s of the 97.13s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.13%)\n\u001b[36m(_ray_fit pid=9861)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=9862)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 5)\n\u001b[36m(_ray_fit pid=9864)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=9861)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 6)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=10054)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=10084)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_ray_fit pid=10084)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 6)\n\u001b[36m(_ray_fit pid=10117)\u001b[0m Metric symmetric_mean_absolute_percentage_error is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=10054)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 6)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0167\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t87.12s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t2.16s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 6.80s of remaining time.\n\u001b[36m(_dystack pid=3096)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_r79_BAG_L2': 0.333, 'ExtraTreesMSE_BAG_L2': 0.2, 'RandomForestMSE_BAG_L2': 0.133, 'CatBoost_BAG_L2': 0.133, 'CatBoost_r177_BAG_L2': 0.133, 'LightGBMLarge_BAG_L1': 0.067}\n\u001b[36m(_dystack pid=3096)\u001b[0m \t-0.0158\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.29s\t = Training   runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m \t0.0s\t = Validation runtime\n\u001b[36m(_dystack pid=3096)\u001b[0m AutoGluon training complete, total runtime = 2693.51s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 32.7 rows/s (4590 batch size)\n\u001b[36m(_dystack pid=3096)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250329_075145/ds_sub_fit/sub_fit_ho\")\n\u001b[36m(_dystack pid=3096)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n\u001b[36m(_dystack pid=3096)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n\u001b[36m(_dystack pid=3096)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n\u001b[36m(_ray_fit pid=10117)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 6)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=3096)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout  score_val                               eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           WeightedEnsemble_L3      -0.015479  -0.015828  symmetric_mean_absolute_percentage_error       65.775586     149.349269  2006.110296                 0.003672                0.001522           0.290561            3       True         27\n1     NeuralNetTorch_r79_BAG_L2      -0.015501  -0.016078  symmetric_mean_absolute_percentage_error       62.087951     144.706290  1770.473597                 0.297531                0.374794         183.521537            2       True         24\n2         NeuralNetTorch_BAG_L2      -0.015565  -0.016210  symmetric_mean_absolute_percentage_error       62.051580     144.715821  1727.394273                 0.261161                0.384325         140.442213            2       True         21\n3                XGBoost_BAG_L2      -0.015624  -0.016038  symmetric_mean_absolute_percentage_error       62.206279     144.646857  1609.323494                 0.415859                0.315361          22.371434            2       True         20\n4               CatBoost_BAG_L2      -0.015636  -0.015986  symmetric_mean_absolute_percentage_error       61.936486     144.577211  1668.967817                 0.146067                0.245715          82.015757            2       True         17\n5          CatBoost_r177_BAG_L2      -0.015640  -0.015990  symmetric_mean_absolute_percentage_error       61.949095     144.540838  1643.358893                 0.158676                0.209342          56.406833            2       True         23\n6          ExtraTreesMSE_BAG_L2      -0.015693  -0.016057  symmetric_mean_absolute_percentage_error       63.474667     146.440592  1604.893770                 1.684248                2.109095          17.941710            2       True         18\n7        RandomForestMSE_BAG_L2      -0.015739  -0.016188  symmetric_mean_absolute_percentage_error       63.485392     146.408802  1665.933898                 1.694973                2.077305          78.981838            2       True         16\n8          LightGBM_r131_BAG_L2      -0.015742  -0.016211  symmetric_mean_absolute_percentage_error       62.391787     145.672344  1602.655293                 0.601368                1.340848          15.703233            2       True         25\n9               LightGBM_BAG_L2      -0.015763  -0.016275  symmetric_mean_absolute_percentage_error       61.907471     144.567455  1597.850029                 0.117051                0.235958          10.897969            2       True         15\n10  NeuralNetFastAI_r191_BAG_L2      -0.015768  -0.016739  symmetric_mean_absolute_percentage_error       62.854597     146.496178  1674.075864                 1.064178                2.164681          87.123804            2       True         26\n11         LightGBMLarge_BAG_L2      -0.015770  -0.016271  symmetric_mean_absolute_percentage_error       62.090808     144.938061  1602.411509                 0.300389                0.606565          15.459449            2       True         22\n12            LightGBMXT_BAG_L2      -0.015789  -0.016186  symmetric_mean_absolute_percentage_error       61.995528     144.757797  1597.481734                 0.205109                0.426300          10.529674            2       True         14\n13          WeightedEnsemble_L2      -0.015792  -0.016029  symmetric_mean_absolute_percentage_error       32.164820      76.888280  1167.738028                 0.002923                0.003661           0.118661            2       True         13\n14         LightGBMLarge_BAG_L1      -0.016204  -0.016568  symmetric_mean_absolute_percentage_error       26.936585      74.228698   101.441511                26.936585               74.228698         101.441511            1       True         11\n15       NeuralNetFastAI_BAG_L2      -0.016249  -0.016546  symmetric_mean_absolute_percentage_error       62.513936     145.523876  1708.556277                 0.723517                1.192379         121.604217            2       True         19\n16              CatBoost_BAG_L1      -0.016415  -0.016982  symmetric_mean_absolute_percentage_error        3.832363       1.366369   647.492920                 3.832363                1.366369         647.492920            1       True          6\n17              LightGBM_BAG_L1      -0.016835  -0.017158  symmetric_mean_absolute_percentage_error        5.917419      14.757148    87.446527                 5.917419               14.757148          87.446527            1       True          4\n18         CatBoost_r177_BAG_L1      -0.017021  -0.017418  symmetric_mean_absolute_percentage_error        0.142479       0.510003    41.633277                 0.142479                0.510003          41.633277            1       True         12\n19        NeuralNetTorch_BAG_L1      -0.017129  -0.017891  symmetric_mean_absolute_percentage_error        0.238055       0.332580   260.673247                 0.238055                0.332580         260.673247            1       True         10\n20            LightGBMXT_BAG_L1      -0.017590  -0.017872  symmetric_mean_absolute_percentage_error       12.807328      42.481109    82.274169                12.807328               42.481109          82.274169            1       True          3\n21               XGBoost_BAG_L1      -0.017592  -0.018012  symmetric_mean_absolute_percentage_error        1.584640       3.629149   163.701223                 1.584640                3.629149         163.701223            1       True          9\n22       NeuralNetFastAI_BAG_L1      -0.018024  -0.019325  symmetric_mean_absolute_percentage_error        1.154894       0.956971   158.011689                 1.154894                0.956971         158.011689            1       True          8\n23       RandomForestMSE_BAG_L1      -0.022041  -0.021983  symmetric_mean_absolute_percentage_error        4.259689       3.491790    36.789379                 4.259689                3.491790          36.789379            1       True          5\n24         ExtraTreesMSE_BAG_L1      -0.024686  -0.024620  symmetric_mean_absolute_percentage_error        4.829682       1.744980     6.676774                 4.829682                1.744980           6.676774            1       True          7\n25        KNeighborsUnif_BAG_L1      -0.038302  -0.037972  symmetric_mean_absolute_percentage_error        0.046362       0.339388     0.716050                 0.046362                0.339388           0.716050            1       True          1\n26        KNeighborsDist_BAG_L1      -0.038950  -0.038777  symmetric_mean_absolute_percentage_error        0.040924       0.493311     0.095294                 0.040924                0.493311           0.095294            1       True          2\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t3202s\t = DyStack   runtime |\t7598s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 7598s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250329_075145\"\nTrain Data Rows:    41308\nTrain Data Columns: 11\nLabel Column:       price\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29627.25 MB\n\tTrain Data (Original)  Memory Usage: 11.30 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', [])  : 1 | ['region_avg_price']\n\t\t('int', [])    : 6 | ['year', 'month', 'day', 'weekday', 'advantage_on_road', ...]\n\t\t('object', []) : 4 | ['region_name', 'plate_number', 'plate_series', 'plate_region']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])  : 4 | ['region_name', 'plate_number', 'plate_series', 'plate_region']\n\t\t('float', [])     : 1 | ['region_avg_price']\n\t\t('int', [])       : 5 | ['year', 'month', 'day', 'weekday', 'significance']\n\t\t('int', ['bool']) : 1 | ['advantage_on_road']\n\t0.4s = Fit runtime\n\t11 features in original data used to generate 11 features in processed data.\n\tTrain Data (Processed) Memory Usage: 1.58 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.43s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'symmetric_mean_absolute_percentage_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 108 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5063.59s of the 7597.27s of remaining time.\n\t-0.0378\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t0.59s\t = Training   runtime\n\t0.16s\t = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5061.27s of the 7594.95s of remaining time.\n\t-0.0385\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t0.04s\t = Training   runtime\n\t0.15s\t = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 5061.06s of the 7594.74s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\t-0.0177\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t70.79s\t = Training   runtime\n\t54.03s\t = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 4982.26s of the 7515.94s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\t-0.017\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t78.61s\t = Training   runtime\n\t48.99s\t = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 4897.59s of the 7431.27s of remaining time.\n\t-0.0218\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t18.87s\t = Training   runtime\n\t2.03s\t = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 4875.15s of the 7408.83s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.06%)\n\t-0.0169\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t685.2s\t = Training   runtime\n\t1.97s\t = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 4186.80s of the 6720.48s of remaining time.\n\t-0.0246\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t8.0s\t = Training   runtime\n\t2.0s\t = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4171.84s of the 6705.52s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\t-0.0194\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t212.49s\t = Training   runtime\n\t1.38s\t = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3955.83s of the 6489.51s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\t-0.0179\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t219.24s\t = Training   runtime\n\t4.16s\t = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3732.47s of the 6266.15s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n\t-0.0178\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t342.6s\t = Training   runtime\n\t0.44s\t = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3386.53s of the 5920.21s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\t-0.0165\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t156.48s\t = Training   runtime\n\t183.82s\t = Validation runtime\nFitting model: CatBoost_r177_BAG_L1 ... Training model for up to 3201.51s of the 5735.19s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.06%)\n\t-0.0169\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t521.77s\t = Training   runtime\n\t1.7s\t = Validation runtime\nFitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 2674.77s of the 5208.45s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n\t-0.0177\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t433.05s\t = Training   runtime\n\t0.41s\t = Validation runtime\nFitting model: LightGBM_r131_BAG_L1 ... Training model for up to 2237.59s of the 4771.27s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\t-0.0166\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t266.71s\t = Training   runtime\n\t433.68s\t = Validation runtime\nFitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1909.25s of the 4442.93s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n\t-0.0186\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t514.34s\t = Training   runtime\n\t2.22s\t = Validation runtime\nFitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1391.37s of the 3925.05s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.10%)\n\t-0.0178\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t324.47s\t = Training   runtime\n\t2.15s\t = Validation runtime\nFitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1062.90s of the 3596.58s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\t-0.0182\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t112.32s\t = Training   runtime\n\t238.23s\t = Validation runtime\nFitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 917.65s of the 3451.33s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n\t-0.0177\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t599.78s\t = Training   runtime\n\t0.71s\t = Validation runtime\nFitting model: XGBoost_r33_BAG_L1 ... Training model for up to 314.28s of the 2847.96s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.25%)\n\t-0.0179\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t262.22s\t = Training   runtime\n\t28.67s\t = Validation runtime\nFitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 44.07s of the 2577.75s of remaining time.\n\t-0.0246\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t8.34s\t = Training   runtime\n\t2.43s\t = Validation runtime\nFitting model: CatBoost_r137_BAG_L1 ... Training model for up to 27.72s of the 2561.40s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n\t-0.0182\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t26.57s\t = Training   runtime\n\t0.31s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 506.36s of the 2531.39s of remaining time.\n\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 0.444, 'CatBoost_r177_BAG_L1': 0.167, 'CatBoost_BAG_L1': 0.111, 'NeuralNetTorch_r79_BAG_L1': 0.111, 'LightGBM_r131_BAG_L1': 0.056, 'NeuralNetFastAI_r191_BAG_L1': 0.056, 'NeuralNetTorch_r22_BAG_L1': 0.056}\n\t-0.0159\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t0.4s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting 106 L2 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 2530.95s of the 2530.86s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.13%)\n\t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t12.98s\t = Training   runtime\n\t0.67s\t = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 2514.25s of the 2514.17s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.13%)\n\t-0.0161\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t14.69s\t = Training   runtime\n\t0.32s\t = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 2495.39s of the 2495.31s of remaining time.\n\t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t161.24s\t = Training   runtime\n\t3.04s\t = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 2327.11s of the 2327.03s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.15%)\n\t-0.0158\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t149.63s\t = Training   runtime\n\t0.31s\t = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 2174.23s of the 2174.14s of remaining time.\n\t-0.0159\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t31.13s\t = Training   runtime\n\t3.19s\t = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2133.93s of the 2133.85s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.19%)\n\t-0.0164\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t160.44s\t = Training   runtime\n\t1.43s\t = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 1970.08s of the 1970.00s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.19%)\n\t-0.0159\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t31.88s\t = Training   runtime\n\t0.47s\t = Validation runtime\nFitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1934.54s of the 1934.46s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.11%)\n\t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t187.4s\t = Training   runtime\n\t0.44s\t = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1743.79s of the 1743.71s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.19%)\n\t-0.0161\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t21.46s\t = Training   runtime\n\t1.08s\t = Validation runtime\nFitting model: CatBoost_r177_BAG_L2 ... Training model for up to 1718.83s of the 1718.75s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.16%)\n\t-0.0158\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t127.61s\t = Training   runtime\n\t0.28s\t = Validation runtime\nFitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 1587.53s of the 1587.45s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.11%)\n\t-0.0159\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t259.17s\t = Training   runtime\n\t0.52s\t = Validation runtime\nFitting model: LightGBM_r131_BAG_L2 ... Training model for up to 1325.06s of the 1324.98s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.20%)\n\t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t35.16s\t = Training   runtime\n\t3.89s\t = Validation runtime\nFitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 1283.73s of the 1283.65s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.20%)\n\t-0.0165\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t318.35s\t = Training   runtime\n\t2.29s\t = Validation runtime\nFitting model: CatBoost_r9_BAG_L2 ... Training model for up to 960.62s of the 960.54s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.27%)\n\t-0.0158\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t145.84s\t = Training   runtime\n\t0.41s\t = Validation runtime\nFitting model: LightGBM_r96_BAG_L2 ... Training model for up to 810.93s of the 810.85s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.12%)\n\t-0.016\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t24.57s\t = Training   runtime\n\t5.14s\t = Validation runtime\nFitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 781.89s of the 781.81s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.11%)\n\t-0.0159\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t308.66s\t = Training   runtime\n\t0.71s\t = Validation runtime\nFitting model: XGBoost_r33_BAG_L2 ... Training model for up to 468.96s of the 468.88s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.47%)\n\t-0.0158\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t86.24s\t = Training   runtime\n\t3.8s\t = Validation runtime\nFitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 378.95s of the 378.87s of remaining time.\n\t-0.0159\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t24.35s\t = Training   runtime\n\t3.1s\t = Validation runtime\nFitting model: CatBoost_r137_BAG_L2 ... Training model for up to 346.84s of the 346.76s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.12%)\n\t-0.0158\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t175.94s\t = Training   runtime\n\t0.29s\t = Validation runtime\nFitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 167.63s of the 167.55s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.20%)\n\t-0.0181\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t72.01s\t = Training   runtime\n\t0.66s\t = Validation runtime\nFitting model: CatBoost_r13_BAG_L2 ... Training model for up to 92.36s of the 92.28s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.27%)\n\t-0.0159\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t78.27s\t = Training   runtime\n\t0.24s\t = Validation runtime\nFitting model: RandomForest_r195_BAG_L2 ... Training model for up to 10.64s of the 10.56s of remaining time.\n\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 11.56s compared to 10s of available time.\n\tTime limit exceeded... Skipping RandomForest_r195_BAG_L2.\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -109.45s of remaining time.\n\tEnsemble Weights: {'NeuralNetTorch_r79_BAG_L2': 0.25, 'NeuralNetTorch_r22_BAG_L2': 0.188, 'RandomForestMSE_BAG_L2': 0.125, 'ExtraTreesMSE_BAG_L2': 0.125, 'CatBoost_r177_BAG_L2': 0.125, 'ExtraTrees_r42_BAG_L2': 0.125, 'CatBoost_r9_BAG_L2': 0.062}\n\t-0.0156\t = Validation score   (-symmetric_mean_absolute_percentage_error)\n\t0.46s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 7707.67s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 5.1 rows/s (5164 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250329_075145\")\n","output_type":"stream"},{"name":"stdout","text":"Performance metrics: {'symmetric_mean_absolute_percentage_error': -0.015592812646959348, 'root_mean_squared_error': -0.5677752721172348, 'mean_squared_error': -0.32236875962780004, 'mean_absolute_error': -0.37660326011933154, 'r2': 0.7614115733699949, 'pearsonr': 0.8727623915162657, 'median_absolute_error': -0.24925136841019935}\nSMAPE: 34.9135\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Prediction 1","metadata":{}},{"cell_type":"code","source":"# # Make predictions on test set (uncomment when ready)\n# test_df = pd.read_csv('/kaggle/input/russian-car-plates-data/test.csv')\n# test_predictions = predictor.predict(test_df)\n# submission = pd.DataFrame({'id': test_df['id'], 'price': test_predictions})\n# submission.to_csv('submission_autogluon.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:44:30.636464Z","iopub.status.idle":"2025-03-29T07:44:30.636837Z","shell.execute_reply":"2025-03-29T07:44:30.636678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pred 2 (sol)","metadata":{}},{"cell_type":"code","source":"from autogluon.tabular import TabularDataset, TabularPredictor\ntest_data = pd.read_csv(\n    \"/kaggle/input/russian-car-plates-data/test.csv\",\n    dtype={\"id\": int, \"plate\": str},\n    parse_dates=[\"date\"],\n)\n\ntest_ids = test_data[\"id\"].copy()\n\nxgb_df_test = process_data(\"/kaggle/input/russian-car-plates-data/test.csv\", region_avg_price_dict)\n\n# Make prediction\ntest_data = TabularDataset(xgb_df_test)\ntest_pred = predictor.predict(test_data)\ntest_pred = np.round(np.expm1(test_pred))\nsubmission = pd.DataFrame()\nsubmission['id'] = test_ids\nsubmission['price'] = test_pred\nsubmission.to_csv('submission.csv',index=False)\nprint('Done producing submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:22:04.591283Z","iopub.execute_input":"2025-03-29T11:22:04.592552Z","iopub.status.idle":"2025-03-29T11:31:57.950135Z","shell.execute_reply.started":"2025-03-29T11:22:04.592508Z","shell.execute_reply":"2025-03-29T11:31:57.948048Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Done producing submission.csv\n","output_type":"stream"}],"execution_count":9}]}